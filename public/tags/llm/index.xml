<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on Muzig 的技术博客</title>
    <link>https://muzig.io/tags/llm/</link>
    <description>Recent content in LLM on Muzig 的技术博客</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 28 Feb 2026 21:30:00 +0800</lastBuildDate>
    <atom:link href="https://muzig.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Karpathy microGPT：200行纯Python实现完整GPT</title>
      <link>https://muzig.io/2026/02/28/karpathy-microgpt200%E8%A1%8C%E7%BA%AFpython%E5%AE%9E%E7%8E%B0%E5%AE%8C%E6%95%B4gpt/</link>
      <pubDate>Sat, 28 Feb 2026 21:30:00 +0800</pubDate>
      <guid>https://muzig.io/2026/02/28/karpathy-microgpt200%E8%A1%8C%E7%BA%AFpython%E5%AE%9E%E7%8E%B0%E5%AE%8C%E6%95%B4gpt/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;em&gt;&amp;ldquo;The most atomic way to train and run inference for a GPT in pure, dependency-free Python. This file is the complete algorithm. Everything else is just efficiency.&amp;rdquo;&lt;/em&gt; — Andrej Karpathy&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;最近 Andrej Karpathy 发布了一个极简 GPT 实现：&lt;strong&gt;microgpt.py&lt;/strong&gt; —— 仅 200 行纯 Python 代码，无任何外部依赖（只用标准库 &lt;code&gt;os&lt;/code&gt;, &lt;code&gt;math&lt;/code&gt;, &lt;code&gt;random&lt;/code&gt;），却包含了完整的训练、推理流程。&lt;/p&gt;&#xA;&lt;p&gt;这不是玩具代码，而是一个&lt;strong&gt;教学杰作&lt;/strong&gt;：它剥离了所有工程复杂性，让你看到 GPT 的算法本质。&lt;/p&gt;</description>
    </item>
    <item>
      <title>llama.cpp vs vLLM：边缘部署与云端服务的架构抉择</title>
      <link>https://muzig.io/2026/02/03/llama.cpp-vs-vllm%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E4%B8%8E%E4%BA%91%E7%AB%AF%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%9E%B6%E6%9E%84%E6%8A%89%E6%8B%A9/</link>
      <pubDate>Tue, 03 Feb 2026 17:45:00 +0800</pubDate>
      <guid>https://muzig.io/2026/02/03/llama.cpp-vs-vllm%E8%BE%B9%E7%BC%98%E9%83%A8%E7%BD%B2%E4%B8%8E%E4%BA%91%E7%AB%AF%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%9E%B6%E6%9E%84%E6%8A%89%E6%8B%A9/</guid>
      <description>&lt;p&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt; 和 &lt;strong&gt;vLLM&lt;/strong&gt; 代表了两种截然不同的设计哲学，选择取决于你的部署场景是&lt;strong&gt;边缘单用户&lt;/strong&gt;还是&lt;strong&gt;云端高并发&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;核心架构差异&#34;&gt;核心架构差异&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;维度&lt;/th&gt;&#xA;          &lt;th&gt;llama.cpp&lt;/th&gt;&#xA;          &lt;th&gt;vLLM&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;设计语言&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;C/C++（零依赖）&lt;/td&gt;&#xA;          &lt;td&gt;Python + CUDA&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;核心创新&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;GGUF 格式 + 内存映射&lt;/td&gt;&#xA;          &lt;td&gt;PagedAttention + 连续批处理&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;内存管理&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;静态分配，mmap 按需加载&lt;/td&gt;&#xA;          &lt;td&gt;动态非连续块分配（类似 OS 虚拟内存）&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;首要目标&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;单流效率与跨平台便携性&lt;/td&gt;&#xA;          &lt;td&gt;多用户吞吐量与 GPU 利用率&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;模型格式&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;GGUF（专用量化格式）&lt;/td&gt;&#xA;          &lt;td&gt;HuggingFace 原生（支持 AWQ/GPTQ）&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;性能特征对比&#34;&gt;性能特征对比&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-吞吐量与并发&#34;&gt;1. 吞吐量与并发&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;vLLM&lt;/strong&gt;：在高并发下吞吐量呈线性增长。&lt;strong&gt;在 H200 上，64 并发时 vLLM 的吞吐量是 llama.cpp 的 35 倍以上&lt;/strong&gt;。支持迭代级调度，GPU 利用率可达 85-92%。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt;：吞吐量几乎不随并发增加而增长（平坦曲线），采用队列模型处理请求。&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-延迟表现&#34;&gt;2. 延迟表现&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;指标&lt;/th&gt;&#xA;          &lt;th&gt;llama.cpp&lt;/th&gt;&#xA;          &lt;th&gt;vLLM&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;首 token 延迟 (TTFT)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;随并发指数级增长（排队等待）&lt;/td&gt;&#xA;          &lt;td&gt;高并发下稳定在 &amp;lt;100ms&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Token 间延迟 (ITL)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;单请求 ITL 更低（无 Python GIL 开销）&lt;/td&gt;&#xA;          &lt;td&gt;为追求吞吐量会略微增加单个请求的 ITL&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;3-内存效率&#34;&gt;3. 内存效率&lt;/h3&gt;&#xA;&lt;p&gt;在 &lt;strong&gt;Jetson Orin Nano (8GB)&lt;/strong&gt; 实测：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
